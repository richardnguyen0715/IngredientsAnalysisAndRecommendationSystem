{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Normalizing Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job description:\n",
    "#### 1. Translate data from Vietnamese to English:\n",
    "\n",
    "a. **kokotaru** 'Assert/cleaned_recipes_translated.txt'\n",
    "\n",
    "b. **kitchenart** 'Assert/cleaned_recipes_2_translated.txt'\n",
    "\n",
    "\n",
    "#### 2. Clean and Normalize data from 'txt' file to 'csv' file\n",
    "\n",
    "   | Name of dish | Ingredient 1  | Ingredient 2  |...            |\n",
    "   | -------------|---------------| --------------|---------------|\n",
    "   | ...          |0/1            |0/1            |...            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\huyen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\huyen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\huyen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\huyen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\huyen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.66.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\huyen\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import shared libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Library_Used import *\n",
    "from Shared_Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from files\n",
    "\n",
    "**black_list.txt**, **units.txt**, **key_words.txt** are txt files containing lists of words that will be removed from data lines to filter out food ingredients.\n",
    "\n",
    "- **black_list.txt** contains noise words to describe the properties and preparation methods of ingredients.\n",
    "\n",
    "- **units.txt** contains words that are units of measurement and quantity of words for an ingredient.\n",
    "\n",
    "- **key_words.txt** is similar to **black_list.txt**, a list of noise words for data, but instead of single words, key words will help you accurately identify clusters of noise words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = read_from_file(\"./Assert/units.txt\",\"r_b_line\")\n",
    "key_words = read_from_file(\"./Assert/key_words.txt\",\"r_b_line\")\n",
    "black_list = read_from_file(\"./Assert/black_list.txt\",\"r_b_line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up tools to use the *nktl* library\n",
    "\n",
    "- Download the necessary data (WordNet and OMW) to create **lemmatizer**.\n",
    "- Create a **lemmatizer** to process words and make them singular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\huyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\huyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ingredient(ingredient: str, units: list, key_words:list, black_list:list):\n",
    "    \"\"\"\n",
    "    Cleans a list of raw words based on specified units, keywords, and blacklist\n",
    "    Parameters:\n",
    "        word (str): a string which maybe contains one or more in it with description.\n",
    "        units (list): contains words that are units of measurement, quantity words\n",
    "        key_words (list): contains words that are signals to identify the noise part in the string\n",
    "        black_list (list): noise words\n",
    "\n",
    "    Returns:\n",
    "        clean_parts(list): list containing one or more cleaned ingredients from the input string 'ingredient'\n",
    "    \"\"\"\n",
    "    # Bước 1: Chuẩn hóa Unicode, đưa chuỗi về kí tự thường\n",
    "    ingredient = unicodedata.normalize(\"NFKD\", ingredient)\n",
    "    ingredient = ingredient.lower()\n",
    "\n",
    "    # Bước 2: thực hiện trước khi tách nguyên liệu: Loại bỏ nội dung trong ngoặc đơn\n",
    "    ingredient = re.sub(r\"\\s*\\(.*?\\)\", \"\", ingredient)\n",
    "    \n",
    "    # Bước 3: Tách thành các nguyên liệu khác nhau khi gặp ',', ';', 'and', 'or', 'with', 'in'\n",
    "    parts = re.split(r\",|;|\\band\\b|\\bor\\b|\\bwith\\b|\\bin\\b\", ingredient, flags=re.IGNORECASE)\n",
    "    clean_parts = []\n",
    "\n",
    "    \n",
    "    # Tiến thành duyệt qua từng phần đã tách được từ bước 3:\n",
    "    for part in parts:\n",
    "        # Bước 4.1: Loại bỏ kí tự không phải ASCII\n",
    "        part = re.sub(r\"[^\\x00-\\x7F]+\", \"\", part)\n",
    "        \n",
    "        # Bước 4.2: Chuẩn hóa các phân số bị lỗi\n",
    "        part = re.sub(r\"\\b\\d+/[a-zA-Z]+\\b\", \"\", part)\n",
    "        \n",
    "        # Bước 4.3: Loại bỏ số lượng và đơn vị đo lường\n",
    "        units_pattern = r\"\\b\\d*[\\d/]*\\s*(\" + \"|\".join(map(re.escape, units)) + r\")\\b\"\n",
    "        part = re.sub(units_pattern, \"\", part, flags=re.IGNORECASE)\n",
    "        part = re.sub(r\"\\d+\", \"\", part) \n",
    "        \n",
    "        # Bước 4.4: Thay thế các ký tự '-', '/', '.', '*' thành khoảng trắng\n",
    "        part = re.sub(r\"[+/.*%>]\", \" \", part)\n",
    "        part = re.sub(r\"(?<=\\s)-|-(?=\\s)|(?<=\\d)-(?=\\d)\", \" \", part)\n",
    "        \n",
    "        # Bước 4.5: Nếu có từ \"of\", chỉ giữ lại phần sau chữ \"of\"\n",
    "        if \" of \" in part:\n",
    "            part = part.split(\" of \", 1)[-1]\n",
    "        \n",
    "        # Bước 4.6: Sử dụng các từ trong key_words để xác định cụm gây nhiễu và xóa chúng\n",
    "        for word in key_words:\n",
    "            pattern = rf\"\\b{re.escape(word)}\\b.*\"\n",
    "            part = re.sub(pattern, \"\", part, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Bước 4.7: Xóa các từ trong black_list mà có xuất hiện trong chuỗi. Chỉ xóa khi từ đó không gắn liền với dấu '-' khi vừa là tên vừa biểu thị đặc tính của nguyên liệu\n",
    "        # (Ví dụ: all-purpose flour : bột mì đa dụng)\n",
    "        for word in black_list:\n",
    "            pattern = rf\"(?<!-)\\b{re.escape(word)}\\b(?!-)\" \n",
    "            part = re.sub(pattern, \"\", part, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Bước 4.8: Xóa khoảng trắng thừa\n",
    "        part = re.sub(r\"\\s+\", \" \", part)\n",
    "        clean_part = part.strip()\n",
    "        \n",
    "        # Bước 4.9: Chuẩn hóa về dạng số ít của nguyên liệu (Ví dụ: apples -> apple)\n",
    "        clean_part = lemmatizer.lemmatize(clean_part)\n",
    "\n",
    "        # Bước 5: Trả về kết quả là một list chứa các nguyên liệu đã được làm sạch.\n",
    "        if clean_part: \n",
    "            clean_parts.append(clean_part)\n",
    "    \n",
    "    return clean_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_plural(word):\n",
    "    \"\"\"\n",
    "    Converts the infinitive form of a word to its plural form\n",
    "    Parameters: word (string)\n",
    "    Returns: plural form of word (string)\n",
    "    \"\"\"\n",
    "    \n",
    "    endings = ('s', 'ss', 'sh', 'ch', 'z', 'x')\n",
    "\n",
    "    if word.endswith('f'):\n",
    "        word = word[:-1] + 'ves'\n",
    "\n",
    "    elif word.endswith('y'):\n",
    "        \n",
    "        if word[-2] in 'aeiou':  \n",
    "            word = word + 's'\n",
    "        else:  \n",
    "            word = word[:-1] + 'ies'\n",
    "    \n",
    "    elif any(word.endswith(ending) for ending in endings):\n",
    "        word = word + 'es'\n",
    "\n",
    "    else:  \n",
    "        word = word + 's'\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_recipes_to_dataframe(file_path, units, key_words, black_list):\n",
    "    \"\"\"\n",
    "    Processes one or more text files of recipes into a binary DataFrame of ingredients.\n",
    "\n",
    "    Parameters:\n",
    "        file_paths (list of str): List of paths to text files containing recipes.\n",
    "        units (list): List of units to clean ingredients.\n",
    "        key_words (list): List of keywords to clean ingredients.\n",
    "        black_list (list): List of blacklisted words to remove from ingredients.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Binary DataFrame with ingredients as columns.\n",
    "        list: List of unique ingredients.\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "\n",
    "    content = read_from_file(file_path, \"r_b_str\")\n",
    "\n",
    "    dishes = content.split('-' * 50)\n",
    "    for dish in dishes:\n",
    "        title_match = re.search(r\"Title:\\s*(.+)\", dish)\n",
    "        ingredients_match = re.search(r\"Ingredients:\\s*(.+)\", dish, re.DOTALL)\n",
    "\n",
    "        if title_match and ingredients_match:\n",
    "\n",
    "            title = title_match.group(1).strip()\n",
    "\n",
    "            ingredients_raw = ingredients_match.group(1).strip()\n",
    "            ingredients_lines = ingredients_raw.splitlines() \n",
    "\n",
    "            if not ingredients_lines:  \n",
    "                combined_data.append({\n",
    "                    'title': title,\n",
    "                    'ingredients': [] \n",
    "                })\n",
    "                continue\n",
    "\n",
    "            clean_ingredients = []\n",
    "            for line in ingredients_lines:\n",
    "                if ':' in line or '=>' in line:\n",
    "                    continue\n",
    "                clean_ingredients.extend(clean_ingredient(line, units, key_words, black_list))\n",
    "\n",
    "            combined_data.append({\n",
    "                'title': title,\n",
    "                'ingredients': clean_ingredients\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(combined_data)\n",
    "\n",
    "    all_ingredients = sorted(set(ingredient.strip().lower()\n",
    "                                 for ingredients in df[\"ingredients\"]\n",
    "                                 for ingredient in ingredients))\n",
    "    return df, all_ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Normalizing data from Kitchenart Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dishes: 682\n",
      "Number of ingredients: 797\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./Assert/cleaned_recipes_2_translated.txt\"\n",
    "df_kitchenart, all_ingredients = normalize_recipes_to_dataframe (file_path,units,key_words,black_list)\n",
    "\n",
    "print(f\"Number of dishes: {df_kitchenart.shape[0]}\")\n",
    "print(f\"Number of ingredients: {len(all_ingredients)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Normalizing data from Kokotaru Website\n",
    "\n",
    "Because the data on ingredients of the dishes collected from the Kokotaru website is extremely complex and has many noise factors, we cannot handle it simply like the Kitchenart website.\n",
    "\n",
    "Here, we will use the list of ingredients obtained from the Kitchenart website to search for those ingredients in the data of the Kokotaru website, then filter it a second time to get new ingredients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find ingredients in all_ingredients that appear in the file 'cleaned_recipes_translated.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path, all_ingredients):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    all_ingredients_sub = sorted(all_ingredients, key=len, reverse=True)        \n",
    "    processed_lines = []\n",
    "    output1_lines = [] \n",
    "    output2_lines = []  \n",
    "    output3_lines = []  \n",
    "    combined_data = []  \n",
    "    in_ingredients_section = False\n",
    "    current_dish_name = None\n",
    "    found_ingredients = []  \n",
    "    output2_dishes = [] \n",
    "\n",
    "    for line in lines:\n",
    "        original_line = line.strip()\n",
    "\n",
    "        if original_line.startswith(\"Title: \"):\n",
    "            if current_dish_name:\n",
    "                if found_ingredients:\n",
    "                    output1_lines.append(f\"Title: {current_dish_name}\\nIngredients:\\n\" + \"\\n\".join(found_ingredients) + \"\\n\" + \"-\" * 50)\n",
    "                    combined_data.append({\n",
    "                        'title': current_dish_name,\n",
    "                        'ingredients': found_ingredients\n",
    "                    })\n",
    "                \n",
    "                if output2_dishes:\n",
    "                    if any(len(dish) > 50 for dish in output2_dishes) or any(\"byDang Ngoc Linh\" in dish for dish in output2_dishes):\n",
    "                        output3_lines.append(f\"Title: {current_dish_name}\\nIngredients:\\n\" + \"\\n\".join(output2_dishes) + \"\\n\" + \"-\" * 50)\n",
    "                    else:\n",
    "                        output2_lines.append(f\"Title: {current_dish_name}\\nIngredients:\\n\" + \"\\n\".join(output2_dishes) + \"\\n\" + \"-\" * 50)\n",
    "\n",
    "            current_dish_name = original_line.split(\"Title: \", 1)[1]\n",
    "            found_ingredients = []  \n",
    "            output2_dishes.clear() \n",
    "            processed_lines.append(original_line)\n",
    "            continue\n",
    "\n",
    "        if original_line.startswith(\"Ingredients:\"):\n",
    "            in_ingredients_section = True\n",
    "            processed_lines.append(original_line)\n",
    "            continue\n",
    "\n",
    "        if original_line == \"-\" * 50:\n",
    "            in_ingredients_section = False\n",
    "            processed_lines.append(original_line)\n",
    "            continue\n",
    "\n",
    "        if in_ingredients_section:\n",
    "            if \":\" in original_line:\n",
    "                processed_lines.append(original_line)\n",
    "                continue\n",
    "\n",
    "            regex_ingredients = []\n",
    "            for ingredient in all_ingredients_sub:\n",
    "                regex_ingredients.append(re.escape(ingredient))  \n",
    "                plural_ingredient = convert_to_plural(ingredient) \n",
    "                if plural_ingredient != ingredient:\n",
    "                    regex_ingredients.append(re.escape(plural_ingredient))\n",
    "\n",
    "           \n",
    "            matched_ingredients = re.findall(r'\\b(?:' + '|'.join(regex_ingredients) + r')\\b', original_line)\n",
    "\n",
    "            filtered_ingredients = []\n",
    "            for m in matched_ingredients:\n",
    "                clean_ingredient = m.strip()\n",
    "\n",
    "                singular_ingredient = lemmatizer.lemmatize(clean_ingredient)\n",
    "                plural_ingredient = convert_to_plural(singular_ingredient) \n",
    "                \n",
    "                if re.search(r'\\bthe\\b\\s+' + re.escape(clean_ingredient), original_line):\n",
    "                    continue \n",
    "\n",
    "                if singular_ingredient in all_ingredients and singular_ingredient not in filtered_ingredients:\n",
    "                    filtered_ingredients.append(singular_ingredient)\n",
    "\n",
    "                elif plural_ingredient in original_line and singular_ingredient not in filtered_ingredients:\n",
    "                    filtered_ingredients.append(singular_ingredient)\n",
    "            \n",
    "            for ingredient in filtered_ingredients:\n",
    "                found_ingredients.append(ingredient)\n",
    "\n",
    "            if not filtered_ingredients:\n",
    "                output2_dishes.append(original_line)\n",
    "\n",
    "            processed_lines.append(original_line)\n",
    "            continue\n",
    "\n",
    "        processed_lines.append(original_line)\n",
    "\n",
    "    if found_ingredients:\n",
    "        output1_lines.append(f\"Title: {current_dish_name}\\nIngredients:\\n\" + \"\\n\".join(found_ingredients) + \"\\n\" + \"-\" * 50)\n",
    "        combined_data.append({\n",
    "            'title': current_dish_name,\n",
    "            'ingredients': found_ingredients\n",
    "        })\n",
    "\n",
    "    if output2_dishes:\n",
    "        if any(len(dish) > 50 for dish in output2_dishes) or any(\"byDang Ngoc Linh\" in dish for dish in output2_dishes):\n",
    "            output3_lines.append(f\"Title: {current_dish_name}\\nIngredients:\\n\" + \"\\n\".join(output2_dishes) + \"\\n\" + \"-\" * 50)\n",
    "        else:\n",
    "            output2_lines.append(f\"Title: {current_dish_name}\\nIngredients:\\n\" + \"\\n\".join(output2_dishes) + \"\\n\" + \"-\" * 50)\n",
    "\n",
    "    with open(\"./Testing/output2.txt\", \"w\", encoding=\"utf-8\") as output2_file:\n",
    "        output2_file.write(\"\\n\".join(output2_lines))\n",
    "\n",
    "    with open(\"./Testing/output3.txt\", \"w\", encoding=\"utf-8\") as output3_file:\n",
    "        output3_file.write(\"\\n\".join(output3_lines))\n",
    "\n",
    "    with open(\"./Testing/output1.txt\", \"w\", encoding=\"utf-8\") as output1_file:\n",
    "        output1_file.write(\"\\n\".join(output1_lines))\n",
    "\n",
    "    df = pd.DataFrame(combined_data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the ingredients from Kokotaru website for the first time by searching based on the ingredients available in \"all_ingredients\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./Assert/cleaned_recipes_translated.txt\"\n",
    "df_kokotaru_1 = process_file(file_path, all_ingredients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuse the \"normalize_recipes_to_dataframe\" function to filter new ingredients from the rows that have not been filtered for ingredients saved in the output2.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_2 = \"./Testing/output2.txt\"\n",
    "df_kokotaru_2, all_ingredients_2 = normalize_recipes_to_dataframe(file_path_2, units,key_words,black_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataframes from two websites Kitchenart and Kokotaru, create binary dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 common problems and mistakes when making bread</td>\n",
       "      <td>[bread]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11 ways to use leftover egg yolks</td>\n",
       "      <td>[cream cheese, chocolate, egg yolk, custard, tea]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12 types of nuts for baking</td>\n",
       "      <td>[skin, seed, hazelnut, chestnut, peanut, walnu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14 ways to make Sponge cake/Gato for birthday ...</td>\n",
       "      <td>[fruit, rice, cake base, cake, whipping cream,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14 ways to use leftover egg whites</td>\n",
       "      <td>[egg white, fruit, chestnut, sweet, chocolate,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>Corn Dessert</td>\n",
       "      <td>[corn, white corn, sugar, salt, tapioca starch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>Sweet and Sour Vegetables (Caponata)</td>\n",
       "      <td>[olive oil, eggplant, shallot, tomato, caper, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>Milk Tea</td>\n",
       "      <td>[milk, sugar, vanilla, cornstarch, pistachio, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>Cheese Chiffon Cake</td>\n",
       "      <td>[milk, cream cheese, unsalted butter, all-purp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>Cold Tofu Pudding</td>\n",
       "      <td>[soy milk, sugar, gelatin powder, water, panda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1057 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     10 common problems and mistakes when making bread   \n",
       "1                     11 ways to use leftover egg yolks   \n",
       "2                           12 types of nuts for baking   \n",
       "3     14 ways to make Sponge cake/Gato for birthday ...   \n",
       "4                    14 ways to use leftover egg whites   \n",
       "...                                                 ...   \n",
       "1052                                       Corn Dessert   \n",
       "1053               Sweet and Sour Vegetables (Caponata)   \n",
       "1054                                           Milk Tea   \n",
       "1055                                Cheese Chiffon Cake   \n",
       "1056                                  Cold Tofu Pudding   \n",
       "\n",
       "                                            ingredients  \n",
       "0                                               [bread]  \n",
       "1     [cream cheese, chocolate, egg yolk, custard, tea]  \n",
       "2     [skin, seed, hazelnut, chestnut, peanut, walnu...  \n",
       "3     [fruit, rice, cake base, cake, whipping cream,...  \n",
       "4     [egg white, fruit, chestnut, sweet, chocolate,...  \n",
       "...                                                 ...  \n",
       "1052  [corn, white corn, sugar, salt, tapioca starch...  \n",
       "1053  [olive oil, eggplant, shallot, tomato, caper, ...  \n",
       "1054  [milk, sugar, vanilla, cornstarch, pistachio, ...  \n",
       "1055  [milk, cream cheese, unsalted butter, all-purp...  \n",
       "1056  [soy milk, sugar, gelatin powder, water, panda...  \n",
       "\n",
       "[1057 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kokotaru = pd.concat([df_kokotaru_1, df_kokotaru_2], ignore_index=True)\n",
    "df_kokotaru = df_kokotaru.groupby('title', as_index=False).agg({\n",
    "    'ingredients': lambda x: list(set().union(*x))  # Hợp nhất các danh sách nguyên liệu\n",
    "})\n",
    "\n",
    "all_ingredients = list(set(all_ingredients+all_ingredients_2))\n",
    "combined_df = pd.concat([df_kokotaru, df_kitchenart], ignore_index=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to binary dataframe; check and merge ingredients that appear in both singular and plural forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary_df(combined_df, all_ingredients):\n",
    "   \n",
    "    binary_df = pd.DataFrame(0, index=combined_df['title'], columns=all_ingredients)\n",
    "    \n",
    "    for _, row in combined_df.iterrows():\n",
    "        ingredients = row['ingredients'] \n",
    "        for ingredient in ingredients:\n",
    "            if ingredient in binary_df.columns:\n",
    "                binary_df.loc[row['title'], ingredient] = 1\n",
    "    \n",
    "    binary_df.insert(0, 'Name of dish', binary_df.index)\n",
    "    binary_df = binary_df.reset_index(drop=True) \n",
    "\n",
    "\n",
    "    colnames = list(binary_df.columns)\n",
    "    for col in colnames:\n",
    "        if col == 'Name of dish':\n",
    "            continue\n",
    "    \n",
    "        plural_col = convert_to_plural(col)\n",
    "        \n",
    "        if plural_col in colnames:\n",
    "            binary_df[col] = binary_df[col] | binary_df[plural_col]\n",
    "            \n",
    "            binary_df.drop(columns=[plural_col], inplace=True)\n",
    "\n",
    "            all_ingredients.remove(plural_col)\n",
    "            all_ingredients = sorted(all_ingredients,key=str.lower)\n",
    "    return binary_df, all_ingredients\n",
    "\n",
    "binary_df, all_ingredients= convert_to_binary_df(combined_df, all_ingredients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dishes: 1057\n",
      "Number of ingredients: 813\n"
     ]
    }
   ],
   "source": [
    "binary_df.to_csv(\"../Assert/ingredients.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "with open(\"./Testing/ingredients.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for ingredient in all_ingredients:\n",
    "        file.write(ingredient + \"\\n\")\n",
    "\n",
    "\n",
    "print(f\"Number of dishes: {binary_df.shape[0]}\")\n",
    "print(f\"Number of ingredients: {len(all_ingredients)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
