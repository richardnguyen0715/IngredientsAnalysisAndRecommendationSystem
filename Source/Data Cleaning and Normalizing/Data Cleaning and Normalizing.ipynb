{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Normalizing Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job description:\n",
    "#### 1. Translate data from Vietnamese to English:\n",
    "\n",
    "a. **kokotaru** 'Assert/cleaned_recipes_translated.txt'\n",
    "\n",
    "b. **kitchenart** 'Assert/cleaned_recipes_2_translated.txt'\n",
    "\n",
    "\n",
    "#### 2. Clean and Normalize data from 'txt' file to 'csv' file\n",
    "\n",
    "   | Name of dish | Ingredient 1  | Ingredient 2  |...            |\n",
    "   | -------------|---------------| --------------|---------------|\n",
    "   | ...          |0/1            |0/1            |...            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\huyen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\huyen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\huyen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\huyen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\huyen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.66.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\huyen\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import shared libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Library_Used import *\n",
    "from Shared_Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from files\n",
    "\n",
    "**black_list.txt**, **units.txt**, **key_words.txt** are txt files containing lists of words that will be removed from data lines to filter out food ingredients.\n",
    "\n",
    "- **black_list.txt** contains noise words to describe the properties and preparation methods of ingredients.\n",
    "\n",
    "- **units.txt** contains words that are units of measurement and quantity of words for an ingredient.\n",
    "\n",
    "- **key_words.txt** is similar to **black_list.txt**, a list of noise words for data, but instead of single words, key words will help you accurately identify clusters of noise words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = read_from_file(\"./Assert/units.txt\",\"r_b_line\")\n",
    "key_words = read_from_file(\"./Assert/key_words.txt\",\"r_b_line\")\n",
    "black_list = read_from_file(\"./Assert/black_list.txt\",\"r_b_line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up tools to use the *nktl* library\n",
    "\n",
    "- Download the necessary data (WordNet and OMW) to create **lemmatizer**.\n",
    "- Create a **lemmatizer** to process words and make them singular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\huyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\huyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ingredient(ingredient: str, units: list, key_words:list, black_list:list,lemmatizer):\n",
    "    \"\"\"\n",
    "    Cleans a list of raw ingredients based on specified units, keywords, and blacklist\n",
    "    Parameters:\n",
    "        ingredient (str): a string which maybe contains one or more in it with description.\n",
    "        units (list): contains words that are units of measurement, quantity words\n",
    "        key_words (list): contains words that are signals to identify the noise part in the string\n",
    "        black_list (list): noise words\n",
    "\n",
    "    Returns:\n",
    "        clean_parts(list): list containing one or more cleaned ingredients from the input string 'ingredient'\n",
    "    \"\"\"\n",
    "    # Bước 1: Chuẩn hóa Unicode, đưa chuỗi về kí tự thường\n",
    "    ingredient = unicodedata.normalize(\"NFKD\", ingredient)\n",
    "    ingredient = ingredient.lower()\n",
    "\n",
    "    # Bước 2: thực hiện trước khi tách nguyên liệu: Loại bỏ nội dung trong ngoặc đơn\n",
    "    ingredient = re.sub(r\"\\s*\\(.*?\\)\", \"\", ingredient)\n",
    "    \n",
    "    # Bước 3: Tách thành các nguyên liệu khác nhau khi gặp ',', ';', 'and', 'or', 'with', 'in'\n",
    "    parts = re.split(r\",|;|\\band\\b|\\bor\\b|\\bwith\\b|\\bin\\b\", ingredient, flags=re.IGNORECASE)\n",
    "    clean_parts = []\n",
    "\n",
    "    \n",
    "    # Tiến thành duyệt qua từng phần đã tách được từ bước 3:\n",
    "    for part in parts:\n",
    "        # Bước 4.1: Loại bỏ kí tự không phải ASCII\n",
    "        part = re.sub(r\"[^\\x00-\\x7F]+\", \"\", part)\n",
    "        \n",
    "        # Bước 4.2: Chuẩn hóa các phân số bị lỗi\n",
    "        part = re.sub(r\"\\b\\d+/[a-zA-Z]+\\b\", \"\", part)\n",
    "        \n",
    "        # Bước 4.3: Loại bỏ số lượng và đơn vị đo lường\n",
    "        units_pattern = r\"\\b\\d*[\\d/]*\\s*(\" + \"|\".join(map(re.escape, units)) + r\")\\b\"\n",
    "        part = re.sub(units_pattern, \"\", part, flags=re.IGNORECASE)\n",
    "        part = re.sub(r\"\\d+\", \"\", part) \n",
    "        \n",
    "        # Bước 4.4: Thay thế các ký tự '-', '/', '.', '*' thành khoảng trắng\n",
    "        part = re.sub(r\"[+/.*>]\", \" \", part)\n",
    "        part = re.sub(r\"(?<=\\s)-|-(?=\\s)|(?<=\\d)-(?=\\d)\", \" \", part)\n",
    "        \n",
    "        # Bước 4.5: Nếu có từ \"of\", chỉ giữ lại phần sau chữ \"of\"\n",
    "        if \" of \" in part:\n",
    "            part = part.split(\" of \", 1)[-1]\n",
    "        \n",
    "        # Bước 4.6: Sử dụng các từ trong key_words để xác định cụm gây nhiễu và xóa chúng\n",
    "        for word in key_words:\n",
    "            pattern = rf\"\\b{re.escape(word)}\\b.*\"\n",
    "            part = re.sub(pattern, \"\", part, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Bước 4.7: Xóa các từ trong black_list mà có xuất hiện trong chuỗi. Chỉ xóa khi từ đó không gắn liền với dấu '-' khi vừa là tên vừa biểu thị đặc tính của nguyên liệu\n",
    "        # (Ví dụ: all-purpose flour : bột mì đa dụng)\n",
    "        for word in black_list:\n",
    "            pattern = rf\"(?<!-)\\b{re.escape(word)}\\b(?!-)\" \n",
    "            part = re.sub(pattern, \"\", part, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Bước 4.8: Xóa khoảng trắng thừa\n",
    "        part = re.sub(r\"\\s+\", \" \", part)\n",
    "        clean_part = part.strip()\n",
    "        \n",
    "        # Bước 4.9: Chuẩn hóa về dạng số ít của nguyên liệu (Ví dụ: apples -> apple)\n",
    "        clean_part = lemmatizer.lemmatize(clean_part)\n",
    "\n",
    "        # Bước 5: Trả về kết quả là một list chứa các nguyên liệu đã được làm sạch.\n",
    "        if clean_part: \n",
    "            clean_parts.append(clean_part)\n",
    "    \n",
    "    return clean_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_recipes_to_dataframe(file_paths, units, key_words, black_list,lemmatizer):\n",
    "    \"\"\"\n",
    "    Processes one or more text files of recipes into a binary DataFrame of ingredients.\n",
    "\n",
    "    Parameters:\n",
    "        file_paths (list of str): List of paths to text files containing recipes.\n",
    "        units (list): List of units to clean ingredients.\n",
    "        key_words (list): List of keywords to clean ingredients.\n",
    "        black_list (list): List of blacklisted words to remove from ingredients.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Binary DataFrame with ingredients as columns.\n",
    "        list: List of unique ingredients.\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "\n",
    "    # Đọc và xử lý từng file\n",
    "    for file_path in file_paths:\n",
    "        # Đọc nội dung file\n",
    "        content = read_from_file(file_path, \"r_b_str\")\n",
    "\n",
    "        # Tách từng món ăn dựa trên dòng gạch ngang\n",
    "        dishes = content.split('-' * 50)\n",
    "        dishes_with_no_ingredients =[]\n",
    "        for dish in dishes:\n",
    "            title_match = re.search(r\"Title:\\s*(.+)\", dish)\n",
    "            ingredients_match = re.search(r\"Ingredients:\\s*(.+)\", dish, re.DOTALL)\n",
    "\n",
    "            if title_match and ingredients_match:\n",
    "                # Lấy tên món ăn\n",
    "                title = title_match.group(1).strip()\n",
    "                # Lấy phần nguyên liệu của món ăn\n",
    "                ingredients_raw = ingredients_match.group(1).strip()\n",
    "                ingredients_lines = ingredients_raw.splitlines() \n",
    "\n",
    "                if not ingredients_lines:  # Nếu danh sách nguyên liệu rỗng\n",
    "                    combined_data.append({\n",
    "                        'title': title,\n",
    "                        'ingredients': []  # Hoặc ['No Ingredients'] nếu bạn muốn ghi chú\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                clean_ingredients = []\n",
    "                for line in ingredients_lines:\n",
    "                    # Bỏ qua các dòng chỉ chú thích về cách thực hiện...\n",
    "                    if ':' in line or '=>' in line:\n",
    "                        continue\n",
    "                    # Làm sạch nguyên liệu\n",
    "                    clean_ingredients.extend(clean_ingredient(line, units, key_words, black_list,lemmatizer))\n",
    "\n",
    "                combined_data.append({\n",
    "                    'title': title,\n",
    "                    'ingredients': clean_ingredients\n",
    "                })\n",
    "\n",
    "    # Tạo DataFrame từ dữ liệu kết hợp\n",
    "    df = pd.DataFrame(combined_data)\n",
    "\n",
    "    # Lấy danh sách nguyên liệu phân biệt\n",
    "    all_ingredients = sorted(set(ingredient.strip().lower()\n",
    "                                 for ingredients in df[\"ingredients\"]\n",
    "                                 for ingredient in ingredients))\n",
    "\n",
    "    # Tạo DataFrame nhị phân\n",
    "    binary_df = pd.DataFrame(columns=[\"Name of dish\"] + all_ingredients)\n",
    "    binary_df[\"Name of dish\"] = df[\"title\"]  # Thêm cột tiêu đề món ăn\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        ingredients_set = set(row[\"ingredients\"])\n",
    "        for ingredient in all_ingredients:\n",
    "            binary_df.at[index, ingredient] = 1 if ingredient in ingredients_set else 0\n",
    "\n",
    "    return binary_df, all_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dishes: 682\n",
      "Number of ingredients: 803\n"
     ]
    }
   ],
   "source": [
    "file_paths = [\"./Assert/cleaned_recipes_2_translated.txt\"]\n",
    "\n",
    "binary_df, all_ingredients = normalize_recipes_to_dataframe (file_paths,units,key_words,black_list,lemmatizer)\n",
    "\n",
    "print(f\"Number of dishes: {binary_df.shape[0]}\")\n",
    "print(f\"Number of ingredients: {binary_df.shape[1]}\")\n",
    "\n",
    "binary_df.to_csv(\"../Assert/ingredients.csv\", index=False, encoding=\"utf-8\")\n",
    "with open(\"./Testing/ingredient_list.txt\",'w') as file:\n",
    "    for item in all_ingredients:\n",
    "        file.write(item + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
