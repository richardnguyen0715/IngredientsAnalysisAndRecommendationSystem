{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Libraries_Used import *\n",
    "from Shared_Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kokotaru_article_links = read_from_file(\"kokotaru_article_urls_first_20.txt\", 'r_b_line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dạng số 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_type1(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            #Title tag\n",
    "            title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "            #Find ingredients\n",
    "            ingredient_div = soup.find(\"div\", class_=\"wprm-recipe-ingredient-group\")\n",
    "            \n",
    "            if ingredient_div:\n",
    "                ingredients = ingredient_div.get_text(separator=\"\\n\").strip()\n",
    "                return title, ingredients  \n",
    "            else:\n",
    "                return title, None \n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_all_ingredients_from_urls_type1(url_list):\n",
    "    ingredients_dict = {}\n",
    "    not_found_urls = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        with tqdm(total=len(url_list), desc=\"Page Loading\", unit=\"page\") as progress_bar:\n",
    "            future_to_url = {executor.submit(fetch_ingredients_type1, url): url for url in url_list}\n",
    "            \n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        title, ingredients = result\n",
    "                        if ingredients:\n",
    "                            ingredients_dict[title] = ingredients\n",
    "                        else:\n",
    "                            not_found_urls.append(url)\n",
    "                    else:\n",
    "                        not_found_urls.append(url)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {url}: {e}\")\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "                time.sleep(2)\n",
    "    \n",
    "    return ingredients_dict, not_found_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ingredients_type1, type_1_not_found = get_all_ingredients_from_urls_type1(kokotaru_article_links)\n",
    "# print(\"Ingredients Dictionary:\", all_ingredients_type1)\n",
    "# print(\"Not Found URLs:\", type_1_not_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dạng số 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_type2(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            # Find title: <h1>, class \"entry-title\"\n",
    "            title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "            # Find ingreditents: <h2>, keyword: \"tỉ lệ\", \"thành phần\", hoặc \"nguyên liệu\" (regardless of case)\n",
    "            ingredient_header = None\n",
    "            for h2 in soup.find_all(\"h2\", class_=\"has-vivid-red-color has-text-color wp-block-heading\"):\n",
    "                h2_text = h2.get_text().strip().lower()  # Lấy văn bản và chuyển thành chữ thường\n",
    "                if any(keyword in h2_text for keyword in [\"tỉ lệ\", \"thành phần\", \"nguyên liệu\"]):\n",
    "                    ingredient_header = h2\n",
    "                    break\n",
    "            \n",
    "            ingredients = None  \n",
    "            \n",
    "            # Get Title\n",
    "            if ingredient_header:\n",
    "                ingredients_list = []\n",
    "                \n",
    "                # Get ingredient list\n",
    "                ul_tag = ingredient_header.find_next(\"ul\")\n",
    "                \n",
    "                if ul_tag:\n",
    "                    for li in ul_tag.find_all(\"li\"):\n",
    "                        ingredients_list.append(li.get_text().strip())\n",
    "                \n",
    "                # List To string\n",
    "                ingredients = \"\\n\".join(ingredients_list) if ingredients_list else None\n",
    "            \n",
    "            if ingredients:\n",
    "                return title, ingredients\n",
    "            else:\n",
    "                return title, None\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_all_ingredients_from_urls_type2(url_list):\n",
    "    ingredients_dict = {}  # {\"Title\" : \"Ingredients\"}\n",
    "    not_found_urls = []     # Urls_Ingredient_not_found\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        with tqdm(total=len(url_list), desc=\"Page Loading\", unit=\"page\") as progress_bar:\n",
    "            future_to_url = {executor.submit(fetch_ingredients_type2, url): url for url in url_list}\n",
    "            \n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        title, ingredients = result\n",
    "                        if ingredients:\n",
    "                            ingredients_dict[title] = ingredients\n",
    "                        else:\n",
    "                            not_found_urls.append(url)\n",
    "                    else:\n",
    "                        not_found_urls.append(url)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {url}: {e}\")\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "                time.sleep(2)\n",
    "    return ingredients_dict, not_found_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ingredients_type2, not_found_type2 = get_all_ingredients_from_urls_type2(type_1_not_found)\n",
    "# print(\"Ingredients Dictionary:\", all_ingredients_type2) \n",
    "# print(\"Not Found URLs:\", not_found_type2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dạng số 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_type3(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "\n",
    "            title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "\n",
    "            ingredient_header = soup.find(\"h3\", string=lambda text: \"Nguyên liệu\" in text if text else False)\n",
    "            \n",
    "            ingredients = None  \n",
    "            \n",
    "            if ingredient_header:\n",
    "                ingredients_list = []\n",
    "                \n",
    "                for sibling in ingredient_header.find_next_siblings():\n",
    "                    if sibling.name == \"h3\":  # Stop when you encounter the next <h3> tag (\"How to\" section)\n",
    "                        break\n",
    "                    if sibling.name == \"p\" and \"text-align\" in sibling.get(\"style\", \"\"):\n",
    "                        ingredients_list.append(sibling.get_text().strip())\n",
    "                \n",
    "                ingredients = \"\\n\".join(ingredients_list) if ingredients_list else None\n",
    "            \n",
    "            return title, ingredients\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_all_ingredients_from_urls_type3(url_list):\n",
    "    ingredients_dict = {}\n",
    "    not_found_urls = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        with tqdm(total=len(url_list), desc=\"Page Loading\", unit=\"page\") as progress_bar:\n",
    "            future_to_url = {executor.submit(fetch_ingredients_type3, url): url for url in url_list}\n",
    "            \n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        title, ingredients = result\n",
    "                        if ingredients:\n",
    "                            ingredients_dict[title] = ingredients\n",
    "                        else:\n",
    "                            not_found_urls.append(url)\n",
    "                    else:\n",
    "                        not_found_urls.append(url)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {url}: {e}\")\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "                time.sleep(2)\n",
    "    \n",
    "    return ingredients_dict, not_found_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ingredients_type3, not_found_type3 = get_all_ingredients_from_urls_type3(not_found_type2)\n",
    "# print(\"Ingredients Dictionary:\", all_ingredients_type3)\n",
    "# print(\"Not Found URLs:\", not_found_type3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dạng số 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_type4(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "            # Find paragraphs containing the keyword \"Ingredients\" with tags like <strong>, <span>, or <h3>\n",
    "            ingredient_header = soup.find(lambda tag: tag.name in [\"strong\", \"span\", \"h3\"] and \"nguyên liệu\" in tag.get_text().lower())\n",
    "            \n",
    "            ingredients = None\n",
    "            \n",
    "            if ingredient_header:\n",
    "                ingredients_list = []\n",
    "                \n",
    "                ul_tag = ingredient_header.find_next(\"ul\")\n",
    "                \n",
    "                if ul_tag:\n",
    "                    for li in ul_tag.find_all(\"li\"):\n",
    "                        ingredients_list.append(li.get_text().strip())\n",
    "                \n",
    "                if not ingredients_list:\n",
    "                    for sibling in ingredient_header.find_next_siblings():\n",
    "                        if sibling.name == \"h3\" or sibling.name == \"strong\":\n",
    "                            break\n",
    "                        if sibling.name == \"p\":\n",
    "                            ingredients_list.append(sibling.get_text().strip())\n",
    "                \n",
    "                ingredients = \"\\n\".join(ingredients_list) if ingredients_list else None\n",
    "            \n",
    "            return title, ingredients\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_all_ingredients_from_urls_type4(url_list):\n",
    "    ingredients_dict = {}\n",
    "    not_found_urls = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        with tqdm(total=len(url_list), desc=\"Page Loading\", unit=\"page\") as progress_bar:\n",
    "            future_to_url = {executor.submit(fetch_ingredients_type4, url): url for url in url_list}\n",
    "            \n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        title, ingredients = result\n",
    "                        if ingredients:\n",
    "                            ingredients_dict[title] = ingredients\n",
    "                        else:\n",
    "                            not_found_urls.append(url)\n",
    "                    else:\n",
    "                        not_found_urls.append(url)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {url}: {e}\")\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "                time.sleep(2)\n",
    "    \n",
    "    return ingredients_dict, not_found_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ingredients_type4, not_found_type4 = get_all_ingredients_from_urls_type4(not_found_type3)\n",
    "# print(\"Ingredients Dictionary:\", all_ingredients_type4)  # In ra dictionary tiêu đề và nguyên liệu\n",
    "# print(\"Not Found URLs:\", not_found_type4)  # In ra danh sách các URL không có nguyên liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_type5(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "            # Find the <h4> tag containing the keyword \"nguyên liệu\"\n",
    "            ingredient_header = soup.find(lambda tag: tag.name == \"h4\" and \"nguyên liệu\" in tag.get_text().lower())\n",
    "            \n",
    "            ingredients = None\n",
    "            \n",
    "            if ingredient_header:\n",
    "                ingredients_list = []\n",
    "                \n",
    "                # Find all <p> tags following the <h4> tag until the next <h3> tag\n",
    "                for sibling in ingredient_header.find_next_siblings():\n",
    "                    if sibling.name == \"h3\":\n",
    "                        break\n",
    "                    if sibling.name == \"p\":\n",
    "                        ingredients_list.append(sibling.get_text().strip())\n",
    "                \n",
    "                # Combine all ingredient lines into a single string\n",
    "                ingredients = \"\\n\".join(ingredients_list) if ingredients_list else None\n",
    "            \n",
    "            return title, ingredients\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_all_ingredients_from_urls_type5(url_list):\n",
    "    ingredients_dict = {}\n",
    "    not_found_urls = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        with tqdm(total=len(url_list), desc=\"Page Loading\", unit=\"page\") as progress_bar:\n",
    "            future_to_url = {executor.submit(fetch_ingredients_type5, url): url for url in url_list}\n",
    "            \n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        title, ingredients = result\n",
    "                        if ingredients:\n",
    "                            ingredients_dict[title] = ingredients\n",
    "                        else:\n",
    "                            not_found_urls.append(url)\n",
    "                    else:\n",
    "                        not_found_urls.append(url)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {url}: {e}\")\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "                time.sleep(2)\n",
    "    \n",
    "    return ingredients_dict, not_found_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ingredients_type5, not_found_type5 = get_all_ingredients_from_urls_type5(not_found_type4)\n",
    "# print(\"Ingredients Dictionary:\", all_ingredients_type5)\n",
    "# print(\"Not Found URLs:\", not_found_type5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_type6(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "            ingredient_start = soup.find(lambda tag: tag.name == \"p\" and \"nguyên liệu\" in tag.get_text().lower())\n",
    "            \n",
    "            ingredients = None\n",
    "            \n",
    "            if ingredient_start:\n",
    "                ingredients_list = []\n",
    "                \n",
    "                for sibling in ingredient_start.find_next_siblings():\n",
    "                    if sibling.name == \"p\" and \"cách làm\" in sibling.get_text().lower():\n",
    "                        break\n",
    "\n",
    "                    if sibling.name == \"p\" and sibling.get_text().strip():\n",
    "                        ingredients_list.append(sibling.get_text().strip())\n",
    "                \n",
    "                ingredients = \"\\n\".join(ingredients_list) if ingredients_list else None\n",
    "            \n",
    "            return title, ingredients\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_all_ingredients_from_urls_type6(url_list):\n",
    "    ingredients_dict = {}\n",
    "    not_found_urls = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        with tqdm(total=len(url_list), desc=\"Page Loading\", unit=\"page\") as progress_bar:\n",
    "            future_to_url = {executor.submit(fetch_ingredients_type6, url): url for url in url_list}\n",
    "            \n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        title, ingredients = result\n",
    "                        if ingredients:\n",
    "                            ingredients_dict[title] = ingredients\n",
    "                        else:\n",
    "                            not_found_urls.append(url)\n",
    "                    else:\n",
    "                        not_found_urls.append(url)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {url}: {e}\")\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "                time.sleep(2)\n",
    "    \n",
    "    return ingredients_dict, not_found_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_type7(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "            ingredient_header = soup.find(lambda tag: tag.name == \"h3\" and \"nguyên liệu\" in tag.get_text().lower())\n",
    "            \n",
    "            ingredients = None\n",
    "            \n",
    "            if ingredient_header:\n",
    "                ingredients_list = []\n",
    "                \n",
    "                for sibling in ingredient_header.find_next_siblings():\n",
    "                    if sibling.name == \"h3\" and \"cách làm\" in sibling.get_text().lower():\n",
    "                        break\n",
    "                    if sibling.name == \"p\" and sibling.get_text().strip():\n",
    "                        ingredients_list.append(sibling.get_text().strip())\n",
    "                \n",
    "                ingredients = \"\\n\".join(ingredients_list) if ingredients_list else None\n",
    "            \n",
    "            return title, ingredients\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_all_ingredients_from_urls_type7(url_list):\n",
    "    ingredients_dict = {}\n",
    "    not_found_urls = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        with tqdm(total=len(url_list), desc=\"Page Loading\", unit=\"page\") as progress_bar:\n",
    "            future_to_url = {executor.submit(fetch_ingredients_type7, url): url for url in url_list}\n",
    "            \n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        title, ingredients = result\n",
    "                        if ingredients:\n",
    "                            ingredients_dict[title] = ingredients\n",
    "                        else:\n",
    "                            not_found_urls.append(url)\n",
    "                    else:\n",
    "                        not_found_urls.append(url)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {url}: {e}\")\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "                time.sleep(2)\n",
    "    \n",
    "    return ingredients_dict, not_found_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_list = ['https://kokotaru.com/blackberry-crumble-cheesecake-bars/']\n",
    "# all_ingredients, not_found = get_all_ingredients_from_urls_type1(u_list)\n",
    "# print(\"Ingredients Dictionary:\", all_ingredients)\n",
    "# print(\"Not Found URLs:\", not_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_with_selenium(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "        title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "        \n",
    "        ingredient_start = soup.find(lambda tag: tag.name == \"p\" and \"nguyên liệu\" in tag.get_text().lower())\n",
    "        \n",
    "        ingredients = None\n",
    "        \n",
    "        if ingredient_start:\n",
    "            ingredients_list = []\n",
    "            \n",
    "            for sibling in ingredient_start.find_next_siblings():\n",
    "                if sibling.name == \"p\" and \"cách làm\" in sibling.get_text().lower():\n",
    "                    break\n",
    "                if sibling.name == \"p\" and sibling.get_text().strip():\n",
    "                    ingredients_list.append(sibling.get_text().strip())\n",
    "            \n",
    "            ingredients = \"\\n\".join(ingredients_list) if ingredients_list else None\n",
    "        \n",
    "        return title, ingredients\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://kokotaru.com/cach-lam-banh-kem-tra-xanh/\"\n",
    "# title, ingredients = fetch_ingredients_with_selenium(url)\n",
    "# print(\"Title:\", title)\n",
    "# print(\"Ingredients:\", ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html_for_diagnostics(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            with open(\"diagnostics.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(response.text)\n",
    "            print(\"Đã lưu HTML vào diagnostics.html để kiểm tra.\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_html_for_diagnostics(\"https://kokotaru.com/cach-lam-banh-kem-tra-xanh/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_and_title(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            title_tag = soup.find(\"div\", class_=\"post-header\").find(\"h1\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "            ingredient_tags = soup.find_all(\"span\", class_=\"ingredient-item__title\")\n",
    "            ingredients = '***'.join(tag.get_text().strip() for tag in ingredient_tags)\n",
    "            \n",
    "            return title, ingredients\n",
    "        else:\n",
    "            return None, None\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None, None\n",
    "\n",
    "def fetch_ingredients_from_multiple_urls(url_list):\n",
    "    all_ingredients = {}\n",
    "    failed_urls = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        with tqdm(total=len(url_list), desc=\"Fetching Ingredients\", unit=\"article\") as progress_bar:\n",
    "            future_to_url = {executor.submit(fetch_ingredients_and_title, url): url for url in url_list}\n",
    "            \n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                title, ingredients = future.result()\n",
    "                \n",
    "                if title and ingredients:\n",
    "                    all_ingredients[title] = ingredients\n",
    "                else:\n",
    "                    failed_urls.append(url)\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "                time.sleep(3)\n",
    "    \n",
    "    print(f\"\\nInitial fetch completed. Failed URLs: {len(failed_urls)}\")\n",
    "    \n",
    "    if failed_urls:\n",
    "        print(\"\\nRetrying failed URLs (1st attempt)...\")\n",
    "        retry_1_failed_urls = []\n",
    "        \n",
    "        with tqdm(total=len(failed_urls), desc=\"Retrying Failed URLs (1st attempt)\", unit=\"article\") as retry_progress_bar:\n",
    "            for url in failed_urls:\n",
    "                title, ingredients = fetch_ingredients_and_title(url)\n",
    "                \n",
    "                if title and ingredients:\n",
    "                    all_ingredients[title] = ingredients\n",
    "                else:\n",
    "                    retry_1_failed_urls.append(url)\n",
    "                \n",
    "                retry_progress_bar.update(1)\n",
    "                time.sleep(3)\n",
    "        \n",
    "        print(f\"\\n1st retry completed. Failed URLs after 1st retry: {len(retry_1_failed_urls)}\")\n",
    "    \n",
    "    if retry_1_failed_urls:\n",
    "        print(\"\\nRetrying failed URLs (2nd attempt)...\")\n",
    "        retry_2_failed_urls = []\n",
    "        \n",
    "        with tqdm(total=len(retry_1_failed_urls), desc=\"Retrying Failed URLs (2nd attempt)\", unit=\"article\") as retry_progress_bar:\n",
    "            for url in retry_1_failed_urls:\n",
    "                title, ingredients = fetch_ingredients_and_title(url)\n",
    "                \n",
    "                if title and ingredients:\n",
    "                    all_ingredients[title] = ingredients\n",
    "                else:\n",
    "                    retry_2_failed_urls.append(url)\n",
    "                \n",
    "                retry_progress_bar.update(1)\n",
    "                time.sleep(3)\n",
    "        \n",
    "        print(f\"\\n2nd retry completed. Final failed URLs: {len(retry_2_failed_urls)}\")\n",
    "    else:\n",
    "        retry_2_failed_urls = []\n",
    "\n",
    "    print(\"\\nFetching Completed\")\n",
    "    print(f\"Total successful pages: {len(all_ingredients)}\")\n",
    "    print(f\"Failed URLs after all retries: {len(retry_2_failed_urls)}\")\n",
    "    \n",
    "    if retry_2_failed_urls:\n",
    "        print(\"\\nFinal Failed URL List:\")\n",
    "        for failed_url in retry_2_failed_urls:\n",
    "            print(f\" - {failed_url}\")\n",
    "\n",
    "    return all_ingredients, retry_2_failed_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = read_from_file('kitchenart_article_urls_first_25.txt', 'r_b_line')\n",
    "all_ingredients, failed_urls = fetch_ingredients_from_multiple_urls(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url, ingredients in all_ingredients.items():\n",
    "    print(f\"\\nIngredients for {url}:\")\n",
    "    print(ingredients)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
