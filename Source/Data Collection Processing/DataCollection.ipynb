{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job description:\n",
    "1. Data scraping website:\\\n",
    "   a. [kokotaru](https://kokotaru.com/) - Total: ~ 500 Posts.\\\n",
    "   b. [kitchenart](https://cook.kitchenart.vn/cong-thuc-nau-an/) - Total: ~ 800 articles.\n",
    "2. Data to scratch:\n",
    "\n",
    "   | Name of dish | Main ingredients     |\n",
    "   | -------------|----------------------|\n",
    "   | ...          |...                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->bs4) (2.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.26.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.27.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nguyendinhtri\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nguyendinhtri\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\nguyendinhtri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\nguyendinhtri\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install selenium\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install requests\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import shared libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Libraries_Used import *\n",
    "from Shared_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comment:**\n",
    "Because these two websites have different ways of operating and interacting, there will be 2 separate information collection sections.\n",
    "1. **Kokotaru:** Interactively scroll to load more articles.\n",
    "2. **Kitchenart:** Divide into separate pages, each page contains 20 articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KOKOTARU WEBSITE ARTICLE URLS PARSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interact:\n",
    "* Load page according to user's mouse scroll action.\n",
    "#### Describe:\n",
    "* All articles are gathered together in one kokotaru homepage link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Retrieve website content using Selenium library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "KOKOTARU_BASE_URL = 'https://kokotaru.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1.1:** Create selenium chrome browser.\\\n",
    "**Step 1.2:** Let the page load all the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kokotaru_page_loader(BASE_URL: str) -> str:\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(BASE_URL)\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    max_scrolls = 100\n",
    "    progress_bar = tqdm(desc=\"Scrolling\", ncols=100, leave=True, unit=\"scroll\", total=max_scrolls)\n",
    "    \n",
    "    scroll_count = 0\n",
    "    \n",
    "    while scroll_count < max_scrolls:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        scroll_count += 1\n",
    "        \n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "    \n",
    "    html_content = driver.page_source\n",
    "    driver.quit()\n",
    "    \n",
    "    progress_bar.close() \n",
    "    print(\"Parsing completed.\")\n",
    "    \n",
    "    return html_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kokotaru_html = kokotaru_page_loader(KOKOTARU_BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1.3:** Get the content into `kokotaru_html_content.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_to_file(\"Assert/kokotaru_html_content.html\",kokotaru_html, 'w_b_str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Get the article links on the homepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kokotaru_content = read_from_file(\"Assert/kokotaru_html_content.html\", 'r_b_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kokotaru_articles_urls(html_content) -> list:\n",
    "    urls = []\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    article_headers = soup.find_all(\"article\", { \"class\" : \"entry-preview\" })\n",
    "    for header in article_headers:\n",
    "        found = header.find('a', class_='cs-overlay-link')\n",
    "        url = found['href'] if found else None\n",
    "        if url is not None:\n",
    "            urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kokotaru_article_urls = get_kokotaru_articles_urls(kokotaru_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the retrieved urls in `kokotaru_aricle_urls.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_to_file('Assert/kokotaru_article_urls.txt', kokotaru_article_urls, 'w_b_element')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KITCHENART WEBSITE ARTICLE URLS PARSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interact:\n",
    "* General pagination.\n",
    "#### Describe:\n",
    "* Each page includes 20 articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get the urls leading to the articles on the current page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kitchenart_article_urls_onepage(page_url: str):\n",
    "    u_list = []\n",
    "\n",
    "    response = requests.get(page_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            articles_list = soup.find_all(\"a\", {\"class\": \"recipe-card__title-link\"})\n",
    "            \n",
    "            for article in articles_list:\n",
    "                link = article.get('href') if article.get('href') else None\n",
    "                if link is not None:\n",
    "                    u_list.append(link)\n",
    "        \n",
    "        except Exception as err:\n",
    "            print(f'Requests error: {err}')\n",
    "    else:\n",
    "        print(f\"Failed to access {page_url}. Status code: {response.status_code}\")\n",
    "        return page_url\n",
    "\n",
    "    return u_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Get the url leading to the next page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 1:** Get the next page through the current page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kitchenart_find_next_page_url(html_content) -> str:\n",
    "    \n",
    "    next_page = \"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_content,\"html.parser\")\n",
    "\n",
    "    link_found = soup.find(\"a\", {\"class\" : \"next page-numbers\"}) if soup.find(\"a\", {\"class\" : \"next page-numbers\"}) else None\n",
    "        \n",
    "    if link_found is not None:\n",
    "        \n",
    "        next_page = link_found.get(\"href\")\n",
    "            \n",
    "        \n",
    "    return next_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 2:** Get all pages from beginning to end, try connecting to that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_page_urls(BASE_URL: str) -> list:\n",
    "    urls = []\n",
    "    failed_urls = []\n",
    "    next_page = BASE_URL\n",
    "    page_counter = 2\n",
    "    \n",
    "    print(\"Starting to gather URLs from the website:\")\n",
    "    progress_bar = tqdm(desc=\"Page Loading\", unit=\"page\")\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(next_page)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            urls.append(next_page)\n",
    "            print(f\"Page {len(urls)} loaded successfully: {next_page}\")\n",
    "            \n",
    "            find_nextpage = soup.find(\"a\", {\"class\": \"next page-numbers\"})\n",
    "            \n",
    "            if find_nextpage is None:\n",
    "                print(\"No more pages to load.\")\n",
    "                break\n",
    "            \n",
    "            next_page = BASE_URL + f'page/{page_counter}/'\n",
    "            page_counter += 1\n",
    "            \n",
    "            time.sleep(3)\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "        else:\n",
    "            failed_urls.append(next_page)\n",
    "            print(f\"Failed to load page {page_counter - 1}: {next_page}\")\n",
    "            break\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    if failed_urls:\n",
    "        print(\"\\nRetrying failed URLs...\")\n",
    "        retry_failed_urls = []\n",
    "        \n",
    "        retry_progress_bar = tqdm(total=len(failed_urls), desc=\"Retrying Failed URLs\", unit=\"page\")\n",
    "        \n",
    "        for url in failed_urls:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                urls.append(url)\n",
    "                print(f\"Retry successful: {url}\")\n",
    "            else:\n",
    "                retry_failed_urls.append(url)\n",
    "                print(f\"Retry failed for: {url}\")\n",
    "            \n",
    "            time.sleep(3)\n",
    "            retry_progress_bar.update(1)\n",
    "        \n",
    "        retry_progress_bar.close()\n",
    "        \n",
    "        if retry_failed_urls:\n",
    "            print(f\"Failed to load {len(retry_failed_urls)} URLs after retrying.\")\n",
    "        else:\n",
    "            print(\"All URLs loaded successfully after retry.\")\n",
    "    \n",
    "    print(f\"\\nTotal pages loaded: {len(urls)}\")\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Retrieve all post urls in all existing pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "KITCHENART_BASE_URL = 'https://cook.kitchenart.vn/cong-thuc-nau-an/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.1:** Get all the page links. If the link is not accessible (error caused by the web owner), the request will be retried 3 times. If it still doesn't work after 3 times, skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_pages = get_all_page_urls(KITCHENART_BASE_URL)\n",
    "# next_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.2:** Write page links into file `kitchenart_page_urls.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_to_file('Assert/kitchenart_pages_urls.txt', next_pages, 'w_b_element')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.3:** The function retrieves all link articles contained in each page. Try to access these pages twice. If the second time still fails, give up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_article_urls_in_all_pages(PAGE_URLS: list) -> list:\n",
    "    u_list = []\n",
    "    failed_urls = []\n",
    "    \n",
    "    progress_bar = tqdm(total=len(PAGE_URLS), desc=\"Page Loading\", unit=\"page\")\n",
    "    \n",
    "    for current_page in PAGE_URLS:\n",
    "        tmp_url = kitchenart_article_urls_onepage(current_page)\n",
    "        \n",
    "        if isinstance(tmp_url, str):\n",
    "            failed_urls.append(current_page)\n",
    "        else:\n",
    "            u_list.extend(tmp_url)\n",
    "        \n",
    "        time.sleep(3)\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    print(f\"Articles found: {len(u_list)}\")\n",
    "    print('---------------------------------')\n",
    "    print(f\"Failed URLs: {len(failed_urls)}\")\n",
    "    \n",
    "    if failed_urls:\n",
    "        print(\"Retry is starting...\")\n",
    "        retry_failed_urls = []\n",
    "        \n",
    "        retry_progress_bar = tqdm(total=len(failed_urls), desc=\"Retrying Failed URLs\", unit=\"page\")\n",
    "        \n",
    "        for current_page in failed_urls:\n",
    "            tmp_url = kitchenart_article_urls_onepage(current_page)\n",
    "            \n",
    "            if isinstance(tmp_url, str):\n",
    "                retry_failed_urls.append(current_page)\n",
    "            else:\n",
    "                u_list.extend(tmp_url)\n",
    "            \n",
    "            time.sleep(3)\n",
    "            retry_progress_bar.update(1)\n",
    "        \n",
    "        retry_progress_bar.close()\n",
    "        print(f\"Articles found after retry: {len(u_list)}\")\n",
    "        print('---------------------------------')\n",
    "        print(f\"Failed URLs after retry: {len(retry_failed_urls)}\")\n",
    "    \n",
    "    print(\"Crawling completed.\")\n",
    "    return u_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run the program to retrieve all article urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_urls = read_from_file('Assert/kitchenart_pages_urls.txt', 'r_b_line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kitchenart_article_urls = find_all_article_urls_in_all_pages(page_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4.1:** Write these urls into the file `kitchenart_article_urls.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_to_file('Assert/kitchenart_aricle_urls.txt', kitchenart_article_urls, 'w_b_element')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KOKOTARU WEBSITE ARTICLE DATA PARSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type 1: Ingredients are presented consistently and have distinguishing tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_type1(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            #Title tag\n",
    "            title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "            #Find ingredients\n",
    "            ingredient_div = soup.find(\"div\", class_=\"wprm-recipe-ingredient-group\")\n",
    "            \n",
    "            if ingredient_div:\n",
    "                ingredients = ingredient_div.get_text(separator=\"\\n\").strip()\n",
    "                return title, ingredients  \n",
    "            else:\n",
    "                return title, None \n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_type2(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            # Find title: <h1>, class \"entry-title\"\n",
    "            title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "            # Find ingreditents: <h2>, keyword: \"tỉ lệ\", \"thành phần\", hoặc \"nguyên liệu\" (regardless of case)\n",
    "            ingredient_header = None\n",
    "            for h2 in soup.find_all(\"h2\", class_=\"has-vivid-red-color has-text-color wp-block-heading\"):\n",
    "                h2_text = h2.get_text().strip().lower()\n",
    "                if any(keyword in h2_text for keyword in [\"tỉ lệ\", \"thành phần\", \"nguyên liệu\"]):\n",
    "                    ingredient_header = h2\n",
    "                    break\n",
    "            \n",
    "            ingredients = None  \n",
    "            \n",
    "            # Get Title\n",
    "            if ingredient_header:\n",
    "                ingredients_list = []\n",
    "                \n",
    "                # Get ingredient list\n",
    "                ul_tag = ingredient_header.find_next(\"ul\")\n",
    "                \n",
    "                if ul_tag:\n",
    "                    for li in ul_tag.find_all(\"li\"):\n",
    "                        ingredients_list.append(li.get_text().strip())\n",
    "                \n",
    "                # List To string\n",
    "                ingredients = \"\\n\".join(ingredients_list) if ingredients_list else None\n",
    "            \n",
    "            if ingredients:\n",
    "                return title, ingredients\n",
    "            else:\n",
    "                return title, None\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_type3(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "\n",
    "            title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "\n",
    "            ingredient_header = soup.find(\"h3\", string=lambda text: \"Nguyên liệu\" in text if text else False)\n",
    "            \n",
    "            ingredients = None  \n",
    "            \n",
    "            if ingredient_header:\n",
    "                ingredients_list = []\n",
    "                \n",
    "                for sibling in ingredient_header.find_next_siblings():\n",
    "                    if sibling.name == \"h3\":  # Stop when you encounter the next <h3> tag (\"How to\" section)\n",
    "                        break\n",
    "                    if sibling.name == \"p\" and \"text-align\" in sibling.get(\"style\", \"\"):\n",
    "                        ingredients_list.append(sibling.get_text().strip())\n",
    "                \n",
    "                ingredients = \"\\n\".join(ingredients_list) if ingredients_list else None\n",
    "            \n",
    "            return title, ingredients\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_type4(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "            # Find paragraphs containing the keyword \"Ingredients\" with tags like <strong>, <span>, or <h3>\n",
    "            ingredient_header = soup.find(lambda tag: tag.name in [\"strong\", \"span\", \"h3\"] and \"nguyên liệu\" in tag.get_text().lower())\n",
    "            \n",
    "            ingredients = None\n",
    "            \n",
    "            if ingredient_header:\n",
    "                ingredients_list = []\n",
    "                \n",
    "                ul_tag = ingredient_header.find_next(\"ul\")\n",
    "                \n",
    "                if ul_tag:\n",
    "                    for li in ul_tag.find_all(\"li\"):\n",
    "                        ingredients_list.append(li.get_text().strip())\n",
    "                \n",
    "                if not ingredients_list:\n",
    "                    for sibling in ingredient_header.find_next_siblings():\n",
    "                        if sibling.name == \"h3\" or sibling.name == \"strong\":\n",
    "                            break\n",
    "                        if sibling.name == \"p\":\n",
    "                            ingredients_list.append(sibling.get_text().strip())\n",
    "                \n",
    "                ingredients = \"\\n\".join(ingredients_list) if ingredients_list else None\n",
    "            \n",
    "            return title, ingredients\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_type5(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "            # Find the <h4> tag containing the keyword \"nguyên liệu\"\n",
    "            ingredient_header = soup.find(lambda tag: tag.name == \"h4\" and \"nguyên liệu\" in tag.get_text().lower())\n",
    "            \n",
    "            ingredients = None\n",
    "            \n",
    "            if ingredient_header:\n",
    "                ingredients_list = []\n",
    "                \n",
    "                # Find all <p> tags following the <h4> tag until the next <h3> tag\n",
    "                for sibling in ingredient_header.find_next_siblings():\n",
    "                    if sibling.name == \"h3\":\n",
    "                        break\n",
    "                    if sibling.name == \"p\":\n",
    "                        ingredients_list.append(sibling.get_text().strip())\n",
    "                \n",
    "                # Combine all ingredient lines into a single string\n",
    "                ingredients = \"\\n\".join(ingredients_list) if ingredients_list else None\n",
    "            \n",
    "            return title, ingredients\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_type6(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "            ingredient_start = soup.find(lambda tag: tag.name == \"p\" and \"nguyên liệu\" in tag.get_text().lower())\n",
    "            \n",
    "            ingredients = None\n",
    "            \n",
    "            if ingredient_start:\n",
    "                ingredients_list = []\n",
    "                \n",
    "                for sibling in ingredient_start.find_next_siblings():\n",
    "                    if sibling.name == \"p\" and \"cách làm\" in sibling.get_text().lower():\n",
    "                        break\n",
    "\n",
    "                    if sibling.name == \"p\" and sibling.get_text().strip():\n",
    "                        ingredients_list.append(sibling.get_text().strip())\n",
    "                \n",
    "                ingredients = \"\\n\".join(ingredients_list) if ingredients_list else None\n",
    "            \n",
    "            return title, ingredients\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_type7(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            title_tag = soup.find(\"h1\", class_=\"entry-title\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "            ingredient_header = soup.find(lambda tag: tag.name == \"h3\" and \"nguyên liệu\" in tag.get_text().lower())\n",
    "            \n",
    "            ingredients = None\n",
    "            \n",
    "            if ingredient_header:\n",
    "                ingredients_list = []\n",
    "                \n",
    "                for sibling in ingredient_header.find_next_siblings():\n",
    "                    if sibling.name == \"h3\" and \"cách làm\" in sibling.get_text().lower():\n",
    "                        break\n",
    "                    if sibling.name == \"p\" and sibling.get_text().strip():\n",
    "                        ingredients_list.append(sibling.get_text().strip())\n",
    "                \n",
    "                ingredients = \"\\n\".join(ingredients_list) if ingredients_list else None\n",
    "            \n",
    "            return title, ingredients\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all ingredients from urls list by type of article format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_ingredients_from_urls(url_list, type):\n",
    "    ingredients_dict = {}\n",
    "    not_found_urls = []\n",
    "    if type == 1:\n",
    "        fetch_ingredients_type = fetch_ingredients_type1\n",
    "    elif type == 2:\n",
    "        fetch_ingredients_type = fetch_ingredients_type2\n",
    "    elif type == 3:\n",
    "        fetch_ingredients_type = fetch_ingredients_type3\n",
    "    elif type == 4:\n",
    "        fetch_ingredients_type = fetch_ingredients_type4\n",
    "    elif type == 5:\n",
    "        fetch_ingredients_type = fetch_ingredients_type5\n",
    "    elif type == 6:\n",
    "        fetch_ingredients_type = fetch_ingredients_type6\n",
    "    elif type == 7:\n",
    "        fetch_ingredients_type = fetch_ingredients_type7\n",
    "    \n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        with tqdm(total=len(url_list), desc=\"Page Loading\", unit=\"page\") as progress_bar:\n",
    "            future_to_url = {executor.submit(fetch_ingredients_type, url): url for url in url_list}\n",
    "            \n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        title, ingredients = result\n",
    "                        if ingredients:\n",
    "                            ingredients_dict[title] = ingredients\n",
    "                        else:\n",
    "                            not_found_urls.append(url)\n",
    "                    else:\n",
    "                        not_found_urls.append(url)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {url}: {e}\")\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "                time.sleep(2)\n",
    "    \n",
    "    return ingredients_dict, not_found_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kokotaru_article_links = read_from_file(\"Assert/kokotaru_article_urls_full.txt\", 'r_b_line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all_ingredients_type1, type_1_not_found = get_all_ingredients_from_urls(kokotaru_article_links, 1)\n",
    "# print(\"Ingredients Dictionary:\", all_ingredients_type1)\n",
    "# print(\"Not Found URLs:\", type_1_not_found)\n",
    "# # print(\"Ingredients Dictionary Len:\", len(all_ingredients_type1))\n",
    "# # print(\"Not Found URLs Len:\", len(type_1_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all_ingredients_type2, type_2_not_found = get_all_ingredients_from_urls(kokotaru_article_links, 2)\n",
    "# print(\"Ingredients Dictionary:\", all_ingredients_type2)\n",
    "# print(\"Not Found URLs:\", type_2_not_found)\n",
    "# # print(\"Ingredients Dictionary Len:\", len(all_ingredients_type2))\n",
    "# # print(\"Not Found URLs Len:\", len(type_2_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ingredients_type3, type_3_not_found = get_all_ingredients_from_urls(kokotaru_article_links, 3)\n",
    "# # print(\"Ingredients Dictionary:\", all_ingredients_type3)\n",
    "# # print(\"Not Found URLs:\", type_3_not_found)\n",
    "# print(\"Ingredients Dictionary Len:\", len(all_ingredients_type3))\n",
    "# print(\"Not Found URLs Len:\", len(type_3_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ingredients_type4, type_4_not_found = get_all_ingredients_from_urls(kokotaru_article_links, 4)\n",
    "# # print(\"Ingredients Dictionary:\", all_ingredients_type4)\n",
    "# # print(\"Not Found URLs:\", type_4_not_found)\n",
    "# print(\"Ingredients Dictionary Len:\", len(all_ingredients_type4))\n",
    "# print(\"Not Found URLs Len:\", len(type_4_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ingredients_type5, type_5_not_found = get_all_ingredients_from_urls(kokotaru_article_links, 5)\n",
    "# # print(\"Ingredients Dictionary:\", all_ingredients_type5)\n",
    "# # print(\"Not Found URLs:\", type_5_not_found)\n",
    "# print(\"Ingredients Dictionary Len:\", len(all_ingredients_type5))\n",
    "# print(\"Not Found URLs Len:\", len(type_5_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ingredients_type6, type_6_not_found = get_all_ingredients_from_urls(kokotaru_article_links, 6)\n",
    "# # print(\"Ingredients Dictionary:\", all_ingredients_type6)\n",
    "# # print(\"Not Found URLs:\", type_6_not_found)\n",
    "# print(\"Ingredients Dictionary Len:\", len(all_ingredients_type6))\n",
    "# print(\"Not Found URLs Len:\", len(type_6_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ingredients_type7, type_7_not_found = get_all_ingredients_from_urls(kokotaru_article_links, 7)\n",
    "# # print(\"Ingredients Dictionary:\", all_ingredients_type7)\n",
    "# # print(\"Not Found URLs:\", type_7_not_found)\n",
    "# print(\"Ingredients Dictionary Len:\", len(all_ingredients_type7))\n",
    "# print(\"Not Found URLs Len:\", len(type_7_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kokotaru_combined_dict = {}\n",
    "# # Kokotaru website\n",
    "# kokotaru_combined_dict.update(all_ingredients_type1)\n",
    "# kokotaru_combined_dict.update(all_ingredients_type2)\n",
    "# kokotaru_combined_dict.update(all_ingredients_type3)\n",
    "# kokotaru_combined_dict.update(all_ingredients_type4)\n",
    "# kokotaru_combined_dict.update(all_ingredients_type5)\n",
    "# kokotaru_combined_dict.update(all_ingredients_type6)\n",
    "# kokotaru_combined_dict.update(all_ingredients_type7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kokotaru_combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"cleaned_recipes.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for title, ingredients in kokotaru_combined_dict.items():\n",
    "#         ingredients = ingredients.replace(\"***\", \"\\n\")\n",
    "#         f.write(f\"Title: {title}\\n\")\n",
    "#         f.write(f\"Ingredients:\\n{ingredients}\\n\")\n",
    "#         f.write(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KITCHENART WEBSITE ARTICLE DATA PARSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ingredients_and_title(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            title_tag = soup.find(\"div\", class_=\"post-header\").find(\"h1\")\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Unknown Title\"\n",
    "            \n",
    "            ingredient_tags = soup.find_all(\"span\", class_=\"ingredient-item__title\")\n",
    "            ingredients = '***'.join(tag.get_text().strip() for tag in ingredient_tags)\n",
    "            \n",
    "            return title, ingredients\n",
    "        else:\n",
    "            return None, None\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None, None\n",
    "\n",
    "def process_batches(url_list, batch_size, delay, max_workers=1):\n",
    "    all_ingredients = {}\n",
    "    failed_urls = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i in range(0, len(url_list), batch_size):\n",
    "            batch = url_list[i:i + batch_size]\n",
    "            future_to_url = {executor.submit(fetch_ingredients_and_title, url): url for url in batch}\n",
    "            \n",
    "            batch_success_count = 0\n",
    "            batch_failure_count = 0\n",
    "            \n",
    "            with tqdm(total=len(batch), desc=f\"Fetching Batch {i // batch_size + 1}\", unit=\"article\") as progress_bar:\n",
    "                for future in as_completed(future_to_url):\n",
    "                    url = future_to_url[future]\n",
    "                    title, ingredients = future.result()\n",
    "                    \n",
    "                    if title and ingredients:\n",
    "                        all_ingredients[title] = ingredients\n",
    "                        batch_success_count += 1\n",
    "                    else:\n",
    "                        failed_urls.append(url)\n",
    "                        batch_failure_count += 1\n",
    "                    \n",
    "                    progress_bar.update(1)\n",
    "                    time.sleep(5)\n",
    "            \n",
    "            # Display results for the completed batch\n",
    "            print(f\"\\nBatch {i // batch_size + 1} completed.\")\n",
    "            print(f\"Successful URLs in this batch: {batch_success_count}\")\n",
    "            print(f\"Failed URLs in this batch: {batch_failure_count}\")\n",
    "            \n",
    "            # Pause between batches\n",
    "            if i + batch_size < len(url_list):\n",
    "                print(f\"Waiting {delay} seconds before next batch...\\n\")\n",
    "                time.sleep(delay)\n",
    "    \n",
    "    return all_ingredients, failed_urls\n",
    "\n",
    "def get_all_ingredients_from_urls_ver2(url_list):\n",
    "    # Initial fetch\n",
    "    print(\"Starting initial fetch...\")\n",
    "    all_ingredients, failed_urls = process_batches(url_list, batch_size=50, delay=180)\n",
    "\n",
    "    print(f\"\\nInitial fetch completed. Failed URLs: {len(failed_urls)}\")\n",
    "    \n",
    "    print(f\"Waiting {180} seconds before Retry 1 for failed URLs...\\n\")\n",
    "    time.sleep(180)\n",
    "    \n",
    "    # Retry 1 for failed URLs\n",
    "    retry_1_failed_urls = []\n",
    "    if failed_urls:\n",
    "        print(\"\\nRetrying failed URLs (1st attempt)...\")\n",
    "        retry_1_ingredients, retry_1_failed_urls = process_batches(failed_urls, batch_size=50, delay=180)\n",
    "        all_ingredients.update(retry_1_ingredients)\n",
    "        print(f\"\\n1st retry completed. Failed URLs after 1st retry: {len(retry_1_failed_urls)}\")\n",
    "\n",
    "    print(f\"Waiting {180} seconds before Retry 2 for failed URLs...\\n\")\n",
    "    time.sleep(180)    \n",
    "    \n",
    "    # Retry 2 for remaining failed URLs\n",
    "    retry_2_failed_urls = []\n",
    "    if retry_1_failed_urls:\n",
    "        print(\"\\nRetrying failed URLs (2nd attempt)...\")\n",
    "        retry_2_ingredients, retry_2_failed_urls = process_batches(retry_1_failed_urls, batch_size=50, delay=180)\n",
    "        all_ingredients.update(retry_2_ingredients)\n",
    "        print(f\"\\n2nd retry completed. Final failed URLs: {len(retry_2_failed_urls)}\")\n",
    "        \n",
    "    print(f\"Waiting {180} seconds before Retry 3 for failed URLs...\\n\")\n",
    "    time.sleep(180)    \n",
    "    \n",
    "    # Retry 2 for remaining failed URLs\n",
    "    retry_3_failed_urls = []\n",
    "    if retry_2_failed_urls:\n",
    "        print(\"\\nRetrying failed URLs (2nd attempt)...\")\n",
    "        retry_3_ingredients, retry_3_failed_urls = process_batches(retry_2_failed_urls, batch_size=50, delay=180)\n",
    "        all_ingredients.update(retry_3_ingredients)\n",
    "        print(f\"\\n2nd retry completed. Final failed URLs: {len(retry_3_failed_urls)}\")\n",
    "\n",
    "    # Summary of results\n",
    "    print(\"\\nFetching Completed\")\n",
    "    print(f\"Total successful pages: {len(all_ingredients)}\")\n",
    "    print(f\"Failed URLs after all retries: {len(retry_3_failed_urls)}\")\n",
    "    \n",
    "    if retry_3_failed_urls:\n",
    "        print(\"\\nFinal Failed URL List:\")\n",
    "        for failed_url in retry_3_failed_urls:\n",
    "            print(f\" - {failed_url}\")\n",
    "\n",
    "    return all_ingredients, retry_3_failed_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list_0 = read_from_file('Assert/kitchenart_article_urls copy 0.txt', 'r_b_line')\n",
    "# all_ingredients_0, failed_urls_0 = get_all_ingredients_from_urls_ver2(url_list_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list_1 = read_from_file('Assert/kitchenart_article_urls copy 1.txt', 'r_b_line')\n",
    "# all_ingredients_1, failed_urls_1 = get_all_ingredients_from_urls_ver2(url_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list_2 = read_from_file('Assert/kitchenart_article_urls copy 2.txt', 'r_b_line')\n",
    "# all_ingredients_2, failed_urls_2 = get_all_ingredients_from_urls_ver2(url_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list_3 = read_from_file('Assert/kitchenart_article_urls copy 3.txt', 'r_b_line')\n",
    "# all_ingredients_3, failed_urls_3 = get_all_ingredients_from_urls_ver2(url_list_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list_4 = read_from_file('Assert/kitchenart_article_urls copy 4.txt', 'r_b_line')\n",
    "# all_ingredients_4, failed_urls_4 = get_all_ingredients_from_urls_ver2(url_list_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list_5 = read_from_file('Assert/kitchenart_article_urls copy 5.txt', 'r_b_line')\n",
    "# all_ingredients_5, failed_urls_5 = get_all_ingredients_from_urls_ver2(url_list_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list_6 = read_from_file('Assert/kitchenart_article_urls copy 6.txt', 'r_b_line')\n",
    "# all_ingredients_6, failed_urls_6 = get_all_ingredients_from_urls_ver2(url_list_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list_7 = read_from_file('Assert/kitchenart_article_urls copy 7.txt', 'r_b_line')\n",
    "# all_ingredients_7, failed_urls_7 = get_all_ingredients_from_urls_ver2(url_list_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list_7 = read_from_file('Assert/kitchenart_article_urls copy 7.txt', 'r_b_line')\n",
    "# all_ingredients_7, failed_urls_7 = get_all_ingredients_from_urls_ver2(url_list_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url_list_8 = read_from_file('Assert/kitchenart_article_urls copy 8.txt', 'r_b_line')\n",
    "#all_ingredients_8, failed_urls_8 = get_all_ingredients_from_urls_ver2(url_list_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list_9 = read_from_file('Assert/kitchenart_article_urls copy 9.txt', 'r_b_line')\n",
    "# all_ingredients_9, failed_urls_9 = get_all_ingredients_from_urls_ver2(url_list_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list_10 = read_from_file('Assert/kitchenart_article_urls copy 10.txt', 'r_b_line')\n",
    "# all_ingredients_10, failed_urls_10 = get_all_ingredients_from_urls_ver2(url_list_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list_11 = read_from_file('Assert/kitchenart_article_urls copy 11.txt', 'r_b_line')\n",
    "# all_ingredients_11, failed_urls_11 = get_all_ingredients_from_urls_ver2(url_list_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list_12 = read_from_file('Assert/kitchenart_article_urls copy 12.txt', 'r_b_line')\n",
    "# all_ingredients_12, failed_urls_12 = get_all_ingredients_from_urls_ver2(url_list_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting initial fetch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Batch 1: 100%|██████████| 50/50 [04:11<00:00,  5.03s/article]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 1 completed.\n",
      "Successful URLs in this batch: 16\n",
      "Failed URLs in this batch: 34\n",
      "\n",
      "Initial fetch completed. Failed URLs: 34\n",
      "Waiting 180 seconds before Retry 1 for failed URLs...\n",
      "\n",
      "\n",
      "Retrying failed URLs (1st attempt)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Batch 1: 100%|██████████| 34/34 [02:50<00:00,  5.01s/article]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 1 completed.\n",
      "Successful URLs in this batch: 5\n",
      "Failed URLs in this batch: 29\n",
      "\n",
      "1st retry completed. Failed URLs after 1st retry: 29\n",
      "Waiting 180 seconds before Retry 2 for failed URLs...\n",
      "\n",
      "\n",
      "Retrying failed URLs (2nd attempt)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Batch 1: 100%|██████████| 29/29 [02:25<00:00,  5.02s/article]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 1 completed.\n",
      "Successful URLs in this batch: 2\n",
      "Failed URLs in this batch: 27\n",
      "\n",
      "2nd retry completed. Final failed URLs: 27\n",
      "Waiting 180 seconds before Retry 3 for failed URLs...\n",
      "\n",
      "\n",
      "Retrying failed URLs (2nd attempt)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Batch 1: 100%|██████████| 27/27 [02:15<00:00,  5.03s/article]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 1 completed.\n",
      "Successful URLs in this batch: 2\n",
      "Failed URLs in this batch: 25\n",
      "\n",
      "2nd retry completed. Final failed URLs: 25\n",
      "\n",
      "Fetching Completed\n",
      "Total successful pages: 25\n",
      "Failed URLs after all retries: 25\n",
      "\n",
      "Final Failed URL List:\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/com-bento-3-mon/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/cooking-show-14-ngo-nuong-chanh-ot-kieu-mexico/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/cooking-show-13-banh-chocolate-5-phut/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/cooking-show-12-cha-ga-goi-la-nep/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/mut-man/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/mut-man-2/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/ca-ro-ran-sot-me-chua-ngot/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/thit-vien-xiu-mai/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/my-y-voi-ca-tim-nuong-cai-bo-xoi-va-pho-mai-ricotta/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/cooking-show-10-pho-mai-que-chien-gion/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/cooking-show-9-nom-sua-ngu-sac/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/kem-que-thanh-long/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/cooking-show-8-hoa-qua-nuong-vani/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/bap-rang-bo-bong-ngo-phu-sot-caramel/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/cooking-show-7-ga-nuong-dau-van-va-ca-chua/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/chua-chua-ngot-ngot-voi-banh-man/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/cooking-show-6-suon-bo-nuong-sot-toi-mat-ong/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/banh-tao-vun/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/panna-cotta-vi-chocolate/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/sorbet-mang-cut/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/12211/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/sangria-cocktail/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/che-buoi-dau-xanh/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/khoai-tay-va-ca-rot-bo-lo/\n",
      " - https://cook.kitchenart.vn/cong-thuc-nau-an/trifle-cam-dau-mat-lanh-don/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "url_list_13 = read_from_file('Assert/kitchenart_article_urls copy 13.txt', 'r_b_line')\n",
    "all_ingredients_13, failed_urls_13 = get_all_ingredients_from_urls_ver2(url_list_13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_list_14 = read_from_file('Assert/kitchenart_article_urls copy 14.txt', 'r_b_line')\n",
    "# all_ingredients_14, failed_urls_14 = get_all_ingredients_from_urls_ver2(url_list_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kitchenart website\n",
    "kitchenart_combined_dict = {}\n",
    "kitchenart_combined_dict.update(all_ingredients_13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cleaned_recipes_13.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    for title, ingredients in kitchenart_combined_dict.items():\n",
    "        ingredients = ingredients.replace(\"***\", \"\\n\")\n",
    "        f.write(f\"Title: {title}\\n\")\n",
    "        f.write(f\"Ingredients:\\n{ingredients}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by dataframe and Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_dict = {}\n",
    "# # combined_dict.update(kokotaru_combined_dict)\n",
    "# combined_dict.update(kitchenart_combined_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"Food_and_Ingredients.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for title, ingredients in kitchenart_combined_dict.items():\n",
    "#         ingredients = ingredients.replace(\"***\", \"\\n\")\n",
    "#         f.write(f\"Title: {title}\\n\")\n",
    "#         f.write(f\"Ingredients:\\n{ingredients}\\n\")\n",
    "#         f.write(\"-\" * 50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
